{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Snq4iq_JOB4T"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **DIABETES PREDICTION(PNDM) USING PYSPARK AND MLLIB**"
      ],
      "metadata": {
        "id": "BvR4ByDG0HEN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snq4iq_JOB4T"
      },
      "source": [
        "#Installing Dependencies & Initiating a New Spark Session\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpCRBcqtN5lH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd2cf87-8c51-4565-c526-71809e55ea6f"
      },
      "source": [
        "#install pyspark\n",
        "! pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=b99b306ecb92aa53b68b9d154deb5e228cb23719dc651488329d23ef56554909\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNZoOuDqOdxX"
      },
      "source": [
        "#creating a sparksession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"diabetes_spark\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sG3MLXlOews"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST1rFFOlOu8s"
      },
      "source": [
        "#create a spark dataframe\n",
        "df = spark.read.csv(\"PNDB.csv\",header=True, inferSchema=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiJm-pmROvKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3504b3-badd-451f-e710-46a69be66d26"
      },
      "source": [
        "#displaying the dataframe\n",
        "df.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "|Age|             HbA1c|Genetic Info|Family History|      Birth Weight|Developmental Delay|     Insulin Level|PNDM|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "|  3|4.8409274670203315|    Mutation|           Yes| 3.128267604571405|                 No| 5.585608291414472|   0|\n",
            "|  3| 5.694742026537993|    Mutation|            No|2.0593417801955525|                 No|3.1413594690450974|   1|\n",
            "|  7| 6.843595441641113| No mutation|            No| 2.718666663895555|                Yes|  4.63931318811706|   0|\n",
            "|  2| 6.480186154058938| No mutation|            No|3.0870165527365243|                 No|6.2171780403504755|   0|\n",
            "|  4| 7.052861345896749|    Mutation|            No| 3.481472357384762|                 No|3.3688923443736773|   0|\n",
            "| 11| 7.978628423869647| No mutation|            No|2.5342488302789747|                 No| 3.869065311785868|   0|\n",
            "| 10| 5.132685176299496| No mutation|            No| 3.630459431421552|                 No| 4.398418457720717|   0|\n",
            "|  7| 5.356342295587941| No mutation|            No|1.9489100564831632|                 No|  6.50843884553458|   0|\n",
            "|  2| 7.539731279886815| No mutation|            No| 2.940289462346059|                 No| 6.746738379442268|   0|\n",
            "|  1| 7.345457764519918| No mutation|            No|3.3983475838551644|                Yes| 6.347773794929128|   0|\n",
            "|  2| 8.704677552983641| No mutation|            No|2.8713200644249803|                 No| 4.592862877713615|   0|\n",
            "| 10| 8.466208381934402| No mutation|            No|2.8798303355271884|                 No| 6.945210485134705|   0|\n",
            "|  1| 7.562560419430341| No mutation|            No| 3.056845028556916|                 No|2.9149935300879073|   0|\n",
            "|  1| 7.937187777452312| No mutation|            No| 2.169505341128035|                Yes|5.7509748681768755|   0|\n",
            "| 10| 7.778588974451081| No mutation|            No| 2.826330882316141|                 No|5.0122764019674015|   0|\n",
            "|  4| 5.526935697271847|    Mutation|            No|2.3989147506783093|                 No| 8.533662967611601|   1|\n",
            "|  5|6.2726494286708965|    Mutation|            No|2.9698629721962666|                 No|1.9667684795900282|   0|\n",
            "|  1| 8.185308139562721|    Mutation|            No| 3.360231762949423|                 No| 7.592557387364564|   0|\n",
            "|  1| 9.142308843478865| No mutation|            No|2.8344828935029263|                Yes| 2.984623727693637|   0|\n",
            "|  5|7.2496841296464245| No mutation|            No| 1.983571868548744|                 No|4.9972042708751285|   0|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape:\", (df.count(), len(df.columns)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQyagEu6YT-Z",
        "outputId": "65f7d274-c8f3-412c-c4af-1f74c1362b8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (100000, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulqkuz8aOvV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a19804c-182e-4de3-a50b-a26bcb9f5646"
      },
      "source": [
        "#printing the schema\n",
        "df.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- HbA1c: double (nullable = true)\n",
            " |-- Genetic Info: string (nullable = true)\n",
            " |-- Family History: string (nullable = true)\n",
            " |-- Birth Weight: double (nullable = true)\n",
            " |-- Developmental Delay: string (nullable = true)\n",
            " |-- Insulin Level: double (nullable = true)\n",
            " |-- PNDM: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZlF2S2Ovgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fde3758-e526-451b-f9b8-98bc0cc3e587"
      },
      "source": [
        "#count the total no. of diabetic and non-diabetic class.\n",
        "print((df.count(),len(df.columns)))\n",
        "df.groupBy('PNDM').count().show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 8)\n",
            "+----+-----+\n",
            "|PNDM|count|\n",
            "+----+-----+\n",
            "|   1| 4822|\n",
            "|   0|95178|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZerPOR-OyjQ"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-KkJvV_PFFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d204d8-58fe-44ab-cf80-8d2fc744bec9"
      },
      "source": [
        "#checking for null values\n",
        "for col in df.columns:\n",
        "  print(col+\":\",df[df[col].isNull()].count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Age: 0\n",
            "HbA1c: 0\n",
            "Genetic Info: 0\n",
            "Family History: 0\n",
            "Birth Weight: 0\n",
            "Developmental Delay: 0\n",
            "Insulin Level: 0\n",
            "PNDM: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"Genetic Info\", when(df[\"Genetic Info\"] == \"No mutation\", 0).otherwise(1))\n",
        "df = df.withColumn(\"Family History\", when(df[\"Family History\"] == \"Yes\", 1).otherwise(0))\n",
        "df = df.withColumn(\"Developmental Delay\", when(df[\"Developmental Delay\"] == \"Yes\", 0).otherwise(1))"
      ],
      "metadata": {
        "id": "TNAe0gA5fYJc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC2XQ7XKfbd2",
        "outputId": "69d4b7fd-352b-4c84-beab-190f1128c445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "|Age|             HbA1c|Genetic Info|Family History|      Birth Weight|Developmental Delay|     Insulin Level|PNDM|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "|  3|4.8409274670203315|           1|             1| 3.128267604571405|                  1| 5.585608291414472|   0|\n",
            "|  3| 5.694742026537993|           1|             0|2.0593417801955525|                  1|3.1413594690450974|   1|\n",
            "|  7| 6.843595441641113|           0|             0| 2.718666663895555|                  0|  4.63931318811706|   0|\n",
            "|  2| 6.480186154058938|           0|             0|3.0870165527365243|                  1|6.2171780403504755|   0|\n",
            "|  4| 7.052861345896749|           1|             0| 3.481472357384762|                  1|3.3688923443736773|   0|\n",
            "| 11| 7.978628423869647|           0|             0|2.5342488302789747|                  1| 3.869065311785868|   0|\n",
            "| 10| 5.132685176299496|           0|             0| 3.630459431421552|                  1| 4.398418457720717|   0|\n",
            "|  7| 5.356342295587941|           0|             0|1.9489100564831632|                  1|  6.50843884553458|   0|\n",
            "|  2| 7.539731279886815|           0|             0| 2.940289462346059|                  1| 6.746738379442268|   0|\n",
            "|  1| 7.345457764519918|           0|             0|3.3983475838551644|                  0| 6.347773794929128|   0|\n",
            "|  2| 8.704677552983641|           0|             0|2.8713200644249803|                  1| 4.592862877713615|   0|\n",
            "| 10| 8.466208381934402|           0|             0|2.8798303355271884|                  1| 6.945210485134705|   0|\n",
            "|  1| 7.562560419430341|           0|             0| 3.056845028556916|                  1|2.9149935300879073|   0|\n",
            "|  1| 7.937187777452312|           0|             0| 2.169505341128035|                  0|5.7509748681768755|   0|\n",
            "| 10| 7.778588974451081|           0|             0| 2.826330882316141|                  1|5.0122764019674015|   0|\n",
            "|  4| 5.526935697271847|           1|             0|2.3989147506783093|                  1| 8.533662967611601|   1|\n",
            "|  5|6.2726494286708965|           1|             0|2.9698629721962666|                  1|1.9667684795900282|   0|\n",
            "|  1| 8.185308139562721|           1|             0| 3.360231762949423|                  1| 7.592557387364564|   0|\n",
            "|  1| 9.142308843478865|           0|             0|2.8344828935029263|                  0| 2.984623727693637|   0|\n",
            "|  5|7.2496841296464245|           0|             0| 1.983571868548744|                  1|4.9972042708751285|   0|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOgSTdBpPs7G"
      },
      "source": [
        "#Performing Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl7Edj-OQACn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb83925-f432-422c-f3f5-09247ad07948"
      },
      "source": [
        "#feature selection\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler= VectorAssembler(inputCols=['Age','HbA1c','Genetic Info','Family History','Birth Weight','Developmental Delay','Insulin Level'],outputCol='features')\n",
        "output_data= assembler.transform(df)\n",
        "output_data.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+--------------------+\n",
            "|Age|             HbA1c|Genetic Info|Family History|      Birth Weight|Developmental Delay|     Insulin Level|PNDM|            features|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+--------------------+\n",
            "|  3|4.8409274670203315|           1|             1| 3.128267604571405|                  1| 5.585608291414472|   0|[3.0,4.8409274670...|\n",
            "|  3| 5.694742026537993|           1|             0|2.0593417801955525|                  1|3.1413594690450974|   1|[3.0,5.6947420265...|\n",
            "|  7| 6.843595441641113|           0|             0| 2.718666663895555|                  0|  4.63931318811706|   0|[7.0,6.8435954416...|\n",
            "|  2| 6.480186154058938|           0|             0|3.0870165527365243|                  1|6.2171780403504755|   0|[2.0,6.4801861540...|\n",
            "|  4| 7.052861345896749|           1|             0| 3.481472357384762|                  1|3.3688923443736773|   0|[4.0,7.0528613458...|\n",
            "| 11| 7.978628423869647|           0|             0|2.5342488302789747|                  1| 3.869065311785868|   0|[11.0,7.978628423...|\n",
            "| 10| 5.132685176299496|           0|             0| 3.630459431421552|                  1| 4.398418457720717|   0|[10.0,5.132685176...|\n",
            "|  7| 5.356342295587941|           0|             0|1.9489100564831632|                  1|  6.50843884553458|   0|[7.0,5.3563422955...|\n",
            "|  2| 7.539731279886815|           0|             0| 2.940289462346059|                  1| 6.746738379442268|   0|[2.0,7.5397312798...|\n",
            "|  1| 7.345457764519918|           0|             0|3.3983475838551644|                  0| 6.347773794929128|   0|[1.0,7.3454577645...|\n",
            "|  2| 8.704677552983641|           0|             0|2.8713200644249803|                  1| 4.592862877713615|   0|[2.0,8.7046775529...|\n",
            "| 10| 8.466208381934402|           0|             0|2.8798303355271884|                  1| 6.945210485134705|   0|[10.0,8.466208381...|\n",
            "|  1| 7.562560419430341|           0|             0| 3.056845028556916|                  1|2.9149935300879073|   0|[1.0,7.5625604194...|\n",
            "|  1| 7.937187777452312|           0|             0| 2.169505341128035|                  0|5.7509748681768755|   0|[1.0,7.9371877774...|\n",
            "| 10| 7.778588974451081|           0|             0| 2.826330882316141|                  1|5.0122764019674015|   0|[10.0,7.778588974...|\n",
            "|  4| 5.526935697271847|           1|             0|2.3989147506783093|                  1| 8.533662967611601|   1|[4.0,5.5269356972...|\n",
            "|  5|6.2726494286708965|           1|             0|2.9698629721962666|                  1|1.9667684795900282|   0|[5.0,6.2726494286...|\n",
            "|  1| 8.185308139562721|           1|             0| 3.360231762949423|                  1| 7.592557387364564|   0|[1.0,8.1853081395...|\n",
            "|  1| 9.142308843478865|           0|             0|2.8344828935029263|                  0| 2.984623727693637|   0|[1.0,9.1423088434...|\n",
            "|  5|7.2496841296464245|           0|             0| 1.983571868548744|                  1|4.9972042708751285|   0|[5.0,7.2496841296...|\n",
            "+---+------------------+------------+--------------+------------------+-------------------+------------------+----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJKrfqdaQAOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ec2b98-6fa1-4eb4-ac73-d648cff4ead6"
      },
      "source": [
        "#print the schema\n",
        "output_data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- HbA1c: double (nullable = true)\n",
            " |-- Genetic Info: integer (nullable = false)\n",
            " |-- Family History: integer (nullable = false)\n",
            " |-- Birth Weight: double (nullable = true)\n",
            " |-- Developmental Delay: integer (nullable = false)\n",
            " |-- Insulin Level: double (nullable = true)\n",
            " |-- PNDM: integer (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fSjOdfyQA99"
      },
      "source": [
        "#Split Dataset & Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F89--FiVQQJn"
      },
      "source": [
        "#create final data\n",
        "final_data = output_data.select('features','PNDM')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-167bVwnjGiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKpm5912Qji_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb398f0-49f7-40c0-b8d7-b7633c010ea7"
      },
      "source": [
        "#print schema of final data\n",
        "final_data.printSchema()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- features: vector (nullable = true)\n",
            " |-- PNDM: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Nom7aZQjuN"
      },
      "source": [
        "#split the dataset and build the model\n",
        "train,test= df.randomSplit([0.7,0.3])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Without HyperParameter Tuning**"
      ],
      "metadata": {
        "id": "dHKhNjorg6ui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DecisionTreeClassifier**"
      ],
      "metadata": {
        "id": "82QFFjvqhPBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "swAMueuGi96C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create a VectorAssembler to assemble the feature columns into a single 'features' column\n",
        "\n",
        "assembler= VectorAssembler(inputCols=['Age','HbA1c','Genetic Info','Family History','Birth Weight','Developmental Delay','Insulin Level'],outputCol='features')\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "decision_tree = DecisionTreeClassifier(labelCol=\"PNDM\", featuresCol=\"features\", maxDepth=4)\n",
        "\n",
        "# Define the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"PNDM\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create a pipeline with the stages\n",
        "dt_pipeline = Pipeline(stages=[assembler, decision_tree])\n",
        "\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "dt_model = dt_pipeline.fit(train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "dt_predict = dt_model.transform(test)\n",
        "\n",
        "# Show the first 5 rows of predictions\n",
        "dt_predict.select(\"PNDM\", \"prediction\").show(5)\n"
      ],
      "metadata": {
        "id": "rg0psqvPj87W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040cd109-0b2f-4711-d7d0-e544f112ed32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|PNDM|prediction|\n",
            "+----+----------+\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   1|       1.0|\n",
            "+----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"PNDM\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Calculate the accuracy\n",
        "dt_auc = evaluator.evaluate(dt_predict)\n",
        "print(\"Decision Tree AUC =\", dt_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSj49rhlkDdX",
        "outputId": "e28cd64d-a4f1-44a6-c701-885cd80b55a5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree AUC = 0.9982503448550992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomForestClassifier**"
      ],
      "metadata": {
        "id": "JNie84APhaAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier"
      ],
      "metadata": {
        "id": "MDImNhs-kRMX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "ran_f = RandomForestClassifier(featuresCol=\"features\", labelCol=\"PNDM\", numTrees=100, maxDepth=4, seed=142)\n",
        "# Define the pipeline stages\n",
        "stages = [assembler, ran_f]\n",
        "\n",
        "# Create a pipeline\n",
        "rf_pipeline = Pipeline(stages=stages)\n",
        "\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "rf_model = rf_pipeline.fit(train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "rf_predictions = rf_model.transform(test)\n",
        "\n",
        "# Show the first 5 rows of predictions\n",
        "rf_predictions.select(\"PNDM\", \"prediction\").show(5)"
      ],
      "metadata": {
        "id": "pnZJ9pMXkRFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d7c126-b369-453e-eca7-692c77d07983"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------+\n",
            "|PNDM|prediction|\n",
            "+----+----------+\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   0|       0.0|\n",
            "|   1|       1.0|\n",
            "+----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Evaluation\n",
        "rf_auc = evaluator.evaluate(rf_predictions)\n",
        "print(\"Random Forest AUC =\", rf_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCBU8lC-kfRH",
        "outputId": "b8f0f305-2bd3-4e43-d8b6-92a5eb15a507"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC = 0.9986556458073372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LogisticRegression**"
      ],
      "metadata": {
        "id": "lBB4urGOhhyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "# Create a Logistic Regression Classifier\n",
        "logistic_reg = LogisticRegression(labelCol=\"PNDM\", featuresCol=\"features\")\n",
        "\n",
        "# Define the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"PNDM\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create a pipeline with the stages\n",
        "lg_pipeline = Pipeline(stages=[assembler, logistic_reg])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "lg_model = lg_pipeline.fit(train)"
      ],
      "metadata": {
        "id": "3e2PhArakyl9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "lg_predictions = lg_model.transform(test)\n",
        "\n",
        "# Evaluate the model\n",
        "log_auc = evaluator.evaluate(lg_predictions)\n",
        "print(\"Logistic Regression AUC = \", log_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtO2dh3Mk44P",
        "outputId": "b1860e96-7f9c-4667-988c-eadee10e1da8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression AUC =  0.9941852848335063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GBTClassifier**"
      ],
      "metadata": {
        "id": "3D3qGw_Wk_RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create a GBT Classifier\n",
        "gbt_classifier = GBTClassifier(labelCol=\"PNDM\", featuresCol=\"features\")\n",
        "\n",
        "# Define the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"PNDM\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create a pipeline with the stages\n",
        "gbt_pipeline = Pipeline(stages=[assembler, gbt_classifier])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "gbt_model = gbt_pipeline.fit(train)\n"
      ],
      "metadata": {
        "id": "DawwK2kLlKSL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "gbt_predictions = gbt_model.transform(test)\n",
        "\n",
        "# Evaluate the model\n",
        "gbt_auc = evaluator.evaluate(gbt_predictions)\n",
        "print(\"GBT Classifier AUC:\", gbt_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUaoIYislPno",
        "outputId": "a8bcbeda-fd23-406e-d796-f6d42ef4b01d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GBT Classifier AUC: 0.9999962077103883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LinearSVC**"
      ],
      "metadata": {
        "id": "egAt1N9nlyAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Create a LinearSVC classifier\n",
        "svc_classifier = LinearSVC(labelCol=\"PNDM\", featuresCol=\"features\")\n",
        "\n",
        "# Define the evaluator\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"PNDM\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "# Create a pipeline with the stages\n",
        "svc_pipeline = Pipeline(stages=[assembler, svc_classifier])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "svc_model = svc_pipeline.fit(train)"
      ],
      "metadata": {
        "id": "P-9nOA0il5bI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "svc_predictions = svc_model.transform(test)\n",
        "\n",
        "# Evaluate the model\n",
        "svc_auc = evaluator.evaluate(svc_predictions)\n",
        "print(\"LinearSVC AUC =\", svc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp-0iEjul-Mb",
        "outputId": "df9f7f60-2a60-4800-a438-5d3dc000cf6e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC AUC = 0.9941409998725846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **With HyperParameter Tuning**"
      ],
      "metadata": {
        "id": "rhsatC_5mISJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target columns\n",
        "feature_column = \"features\"\n",
        "target_column = \"PNDM\""
      ],
      "metadata": {
        "id": "ci9jKqmsmUgn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DecisionTreeClassifier**"
      ],
      "metadata": {
        "id": "0SYo7h8smXeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
      ],
      "metadata": {
        "id": "Jkmu2bWlmWud"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(labelCol=target_column, featuresCol=feature_column)\n",
        "\n",
        "# Create a ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(dt.maxDepth, [5, 10, 15])  # Maximum depth of the tree\n",
        "    .addGrid(dt.impurity, ['gini', 'entropy'])  # Criterion for information gain\n",
        "    .build())\n",
        "\n",
        "# Train-validation split\n",
        "tvs_dt = TrainValidationSplit(\n",
        "    estimator=dt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "UUe4AIC4m-eP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with grid search\n",
        "dt_model_tuned = tvs_dt.fit(train)\n",
        "best_dt_model = dt_model_tuned.bestModel\n",
        "\n",
        "# Make predictions and evaluate\n",
        "best_dt_predictions = best_dt_model.transform(test)\n",
        "best_dt_auc = evaluator.evaluate(best_dt_predictions)\n",
        "print(\"Tuned Decision Tree AUC =\", best_dt_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oahJ-sI9nDHl",
        "outputId": "c7b5c720-9e63-40ae-8e2c-fb4158776782"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Decision Tree AUC = 0.9997546613164778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RandomForestClassifier**"
      ],
      "metadata": {
        "id": "TZuyyqoyofiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier"
      ],
      "metadata": {
        "id": "sZRJWf5Solbk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=target_column, featuresCol=feature_column)\n",
        "\n",
        "# Create a ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(rf.numTrees, [10, 20, 30])\n",
        "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
        "    .build())\n",
        "\n",
        "# Train-validation split\n",
        "tvs = TrainValidationSplit(\n",
        "    estimator=rf,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "uhmnLdFhoqml"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# List all your feature columns (excluding the label/target column)\n",
        "feature_columns = ['Age', 'HbA1c', 'Genetic Info', 'Family History',\n",
        "                   'Birth Weight', 'Developmental Delay', 'Insulin Level']\n",
        "\n",
        "# Drop the existing \"features\" column if it exists\n",
        "if \"features\" in train.columns:\n",
        "    train = train.drop(\"features\")\n",
        "if \"features\" in test.columns:\n",
        "    test = test.drop(\"features\")\n",
        "\n",
        "# Create the VectorAssembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_columns,\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Transform your training and test data\n",
        "train = assembler.transform(train)\n",
        "test = assembler.transform(test)"
      ],
      "metadata": {
        "id": "E7cbVjn9oxOA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LogisticRegression**"
      ],
      "metadata": {
        "id": "jgi8DcISqnGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "metadata": {
        "id": "K-Tzc2JMqrXZ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "lr = LogisticRegression(labelCol=target_column, featuresCol=feature_column)\n",
        "\n",
        "# Create a ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(lr.regParam, [0.01, 0.1, 1.0])\n",
        "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "    .build())\n",
        "\n",
        "# Train-validation split\n",
        "tvs_lr = TrainValidationSplit(\n",
        "    estimator=lr,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "-kcjlwRVqsET"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with grid search\n",
        "lr_model_tuned = tvs_lr.fit(train)\n",
        "best_lr_model = lr_model_tuned.bestModel\n",
        "\n",
        "# Make predictions and evaluate\n",
        "best_lr_predictions = best_lr_model.transform(test)\n",
        "best_lr_auc = evaluator.evaluate(best_lr_predictions)\n",
        "print(\"Tuned Logistic Regression AUC =\", best_lr_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_NBy2CBqusR",
        "outputId": "a5427b27-4b88-4348-b30a-e797c7a4fba3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned Logistic Regression AUC = 0.9941830643481447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GBTClassifier**"
      ],
      "metadata": {
        "id": "5_br3VAlq8H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier"
      ],
      "metadata": {
        "id": "I-4CdBj0rAwj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient-Boosted Trees (GBT) Classifier\n",
        "gbt = GBTClassifier(labelCol=target_column, featuresCol=feature_column)\n",
        "\n",
        "# Create a ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(gbt.maxDepth, [5, 10, 15])\n",
        "    .addGrid(gbt.maxIter, [10, 20, 30])\n",
        "    .build())\n",
        "\n",
        "# Train-validation split\n",
        "tvs_gbt = TrainValidationSplit(\n",
        "    estimator=gbt,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "RxPDRwYurKzJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with grid search\n",
        "gbt_model_tuned = tvs_gbt.fit(train)\n",
        "best_gbt_model = gbt_model_tuned.bestModel\n",
        "\n",
        "# Make predictions and evaluate\n",
        "best_gbt_predictions = best_gbt_model.transform(test)\n",
        "best_gbt_auc = evaluator.evaluate(best_gbt_predictions)\n",
        "print(\"Tuned GBT Classifier AUC =\", best_gbt_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4EcuAdfrajE",
        "outputId": "e1b31dfe-5336-4cf4-89a1-a529116b356e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned GBT Classifier AUC = 0.9999968064929587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **LinearSVC**"
      ],
      "metadata": {
        "id": "aZlCk2wYrkBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC"
      ],
      "metadata": {
        "id": "RYmshWFNrmU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Support Vector Machine (SVM)\n",
        "svm = LinearSVC(labelCol=target_column, featuresCol=feature_column, maxIter=10)\n",
        "\n",
        "# Create a ParamGrid for hyperparameter tuning\n",
        "paramGrid = (ParamGridBuilder()\n",
        "    .addGrid(svm.regParam, [0.01, 0.1, 1.0])\n",
        "    .build())\n",
        "\n",
        "# Train-validation split\n",
        "tvs_svm = TrainValidationSplit(\n",
        "    estimator=svm,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=evaluator,\n",
        "    trainRatio=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "yb_hSnVqroGK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model with grid search\n",
        "svm_model_tuned = tvs_svm.fit(train)\n",
        "best_svm_model = svm_model_tuned.bestModel\n",
        "\n",
        "# Make predictions and evaluate\n",
        "best_svm_predictions = best_svm_model.transform(test)\n",
        "best_svm_auc = evaluator.evaluate(best_svm_predictions)\n",
        "print(\"Tuned SVM AUC =\", best_svm_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It08V87TrrXk",
        "outputId": "63d54b79-2018-4b31-9125-b0465a28bae3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned SVM AUC = 0.9940603139212432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zG9ssuU3mI5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}